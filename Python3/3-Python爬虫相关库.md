





# Python爬虫相关的库



Python爬虫的基本流程是：

1. 确定目标，就是你需要爬哪些信息
2. 网页定位，就是哪个网页的哪个位置有你需要的信息
3. 确定路径，就是要到达那个位置应该怎么走。
4. 代码实现，就是通过代码实现爬虫的整个流程： **发送请求——获得页面——解析页面——下载内容——储存内容。**

在这个过程中，你可能需要一系列的工具。
**爬虫准备工具：帮你定位你要的信息**
F12: 浏览器开发者工具
httpfox: 抓包工具
xpath checker: xpath工具
正则表达式测试：[https://tool.oschina.net/regex/](https://links.jianshu.com/go?to=https%3A%2F%2Ftool.oschina.net%2Fregex%2F)

**请求库：实现 HTTP 请求操作**
urllib：一系列用于操作URL的功能。
requests：基于 urllib 编写的，阻塞式 HTTP 请求库，发出一个请求，一直等待服务器响应后，程序才能进行下一步处理。
selenium：自动化测试工具。一个调用浏览器的 driver，通过这个库你可以直接调用浏览器完成某些操作，比如输入验证码。

**解析库：从网页中提取信息**
beautifulsoup：html 和 XML 的解析,从网页中提取信息，同时拥有强大的API和多样解析方式。
lxml：支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高。
tesserocr：一个 OCR 库，在遇到验证码（图形验证码为主）的时候，可直接用 OCR 进行识别。

**爬虫框架**
Scrapy：很强大的爬虫框架，可以满足简单的页面爬取（比如可以明确获知url pattern的情况）。用这个框架可以轻松爬下来如亚马逊商品信息之类的数据。但是对于稍微复杂一点的页面，如 weibo 的页面信息，这个框架就满足不了需求了。

Pyspider：是由国人(binux)编写的强大的网络爬虫系统，它带有强大的WebUi / 脚本编辑器 / 任务监控器 / 项目管理器以及结果处理器。他支持多种数据库后端 / 多种消息队列 / Javascript 渲染页面爬去。使用起来非常方便









> 参考资料：
>
> [1]https://www.jianshu.com/p/6ba8b4085a36

